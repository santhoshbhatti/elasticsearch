####################################################
URLS:

https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html
####################################################



The docs to install elastic search:

 it details the installation of 
1. Virtualbox
2. Creating a ubuntu box
3. installing the Elastic search
4. starting elasticsearch as a service in the ubuntu machine
5. testing the installation
6. importing a sechema for indexing all shakespeare's works
7. imporing the all shakespeare's works and indexing them
8. Searching for a phrase such as to be or not to be

Installation:

While installation change the network.host to 0.0.0.0
This solves the probem of connection reset by peer problem -----very important!!!!!!!!
--------------------------------------------------------------------------------------------------
importing a sechema for indexing all shakespeare's works

getting the schema downloaded from given url:

wget http://media.sundog-soft.com/es/shakes-mapping.json


imporing the schema to elasticseach instance:

curl -XPUT 127.0.0.1:9200/shakespeare --data-binary @shakes-mapping.json

########################################################################################################
the mapping 

{
	"mappings" : {
 		"_default_" : {
			"properties" : {
				"speaker" : {"type": "string", "index" : "not_analyzed" },
				"play_name" : {"type": "string", "index" : "not_analyzed" },
				"line_id" : { "type" : "integer" },
				"speech_number" : { "type" : "integer" }
			}
		}
	}
}

########################################################################################################

Document indexed:


{
   "line_id":14,
   "play_name":"Henry IV",
   "speech_number":1,
   "line_number":"1.1.11",
   "speaker":"KING HENRY IV",
   "text_entry":"All of one nature, of one substance bred,"
}








########################################################################################################

getting all shakespeare's work downloded from below url as follows:

wget http://media.sundog-soft.com/es/shakespeare.json

Indexing the works of shakespeare in the running instance of elasticsearch:

curl -XPOST '127.0.0.1:9200/shakespeare/_bulk' --data-binary @shakespeare.json  


Summary imporing a new schema in elastic search is a PUT request and indexing ie..imporing the
documents to be indexed to elasticsearch is a POST request  

Searching the indexed data is as follows:

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

curl -XGET '127.0.0.1:9200/shakespeare/_search?pretty' -d '
{
"query" : {
"match_phrase" : {
"text_entry" : "to be or not to be"
}
}
}
'

searching is a get request----get request with a body*
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

------------------------------------------------------------------------------------------------------

Indexing the movie dataset from movielens:

This is a simple schema definition or mapping...here 
the movie has many other fields TODO define all fields which are part of the documents......
Elastic search figures out what to do with the fields by default...ie....how to index/analyze the fields
But in certian cases we need to give it hints like below....we have disabled _all field....this field will hold
 all fields if we want to search on any field
But the mapping below 

################################################################################
Example mapping for the movie index 

curl -XPUT 127.0.0.1:9200/movies -d '
{
	"mappings":{
		"movie":{
			"_all":{"enabled":false},
			"properties":{
				"year":{"type":"date"}
			}
		}
	}
}  
'
other fields are title,year and genre


In the above example the index we are creating is the movies ---part of the url /movies
and the type we are mapping here is movie inside "mappings" we have type mentioned as "movie"

response : {"acknowledged":true,"shards_acknowledged":true,"index":"movies"}

How to retrive the mapping from the elasticsearch:

curl -XGET 127.0.0.1:9200/movies/_mapping/movie?pretty 

?pretty will print formatted json as follws: response of above rest call:

{
  "movies" : {
    "mappings" : {
      "movie" : {
        "_all" : {
          "enabled" : false
        },
        "properties" : {
          "year" : {
            "type" : "date"
          }
        }
      }
    }
  }
}


################################################################################


Mappings or schema definition :
 Very important while indexing data , how do we define a document having fields

_all field is introduced by elastic search in all the docs which are indexed by default....this field combines
all the fields togeather...to give us the capability to search accross all fields at once.

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Mappings can  define the following things 
1. types ----string,byte,short,integer,long,float,double,boolean,date
ex---- 
here we can mention specifically what the fields type is 

"property":{
	"user_id":{"type":"long"}
}

2. Field index ----gives us ability to control how a field is indexed ----Analyled/not Analyzed/not indexed
The question we ask here is do you want this field to be indexed for full text search? Analyled/not Analyzed/not 
example: 
"property" : {
	"genre" : {
		"index":"not_analyzed"
	}
}

in the above property definition Index the field but do not apply any analyzer----by default 
elastic search provides a analyzer depending on the type of the field

What do do we mean by analyzed? Details later---TODO get the details
Analyzed means ....how field is tokenized for full text search.
If we have enabled analyzed....elasticsearch indexing will break the field up into terms and create a inverted index of those terms.
This is not required for all data as this kind of tokenization is not relevent for some fields.....like the above field genre
if the genere is science fiction we need not tokenize this string to science and fiction. we would always search the genre as science fiction 
and not as individual terms....hence not_analyzed.

If we dont want to search the documents on a field such fields are annotated/mapped index as not.-----such fields will come back as part 
of the documents which are retrived during a search ----search on a different field 

3. field analyzer:

define your tokenizer and token filter ---standard/simple/english/whitespace etc

eg :
"property": {
	"description":{
		"analyzer" : "english"
	}
}

Interpret the field value of description as english text....do english specific things like synonyms, stemming, language specific stopwords
 and others.
 More details shortly

More about Analyzers : Important 

three things analyzers can do:  These are things analyzers can do

1. Caharacter filters
	remove HTML encoding , convert & to and etc
The idea is if we apply same analyzer to the data being indexed and search query we can get rid of such descrepancies. 
ie if we search for  & or and we get results

2. Tokenizer 
	Split strings on whitespace/punctuation/non-letters ...language specific tokenization
        the choice of tokenizer will determine how a text gets broken up into search terms that get indexed.

3. Token Filtereing:
	lower casing/synonyms/stemming/stop words/
If we apply same analyzer to both the data being indexed and the search query we can get rid of case sensitivity or
 we can search using a synonym. 

Stemming will reduce a term to its base meaning ie boxes, boxing, boxed will result to term box ----normalize different variants
 of a given word to the same root stem.

Synonyms filter if applied both in indexing and querying we can get results if the docs contain synonyms ...eg if we search
for big we may get docs with large or huge in it

Stopwords filter if applied will stop some common words which do not convey much meaning like "the, to, or" will not
be indexed and niter be searched...dont enable stopwords lightly....usually in phrase searches if we had 
filtered stop words we may not get
desired results...like in "to be or not to be" our last example if we had enabled stopword filtering
we would not get any results on the phrase search as all words in the given phrase are stop words.

Types of Analyzers:

Standard :
	Splits on word boundaries, removes punctuation, lowercases. good choice when the language is unknown.
	we are using this analyzer for movie titles as we have some foreign language movies in the data set
Simple : 
	Splits on anything that isn't a letter and lowercases
Whitespace:
	splits on whitespace but doesn't lowecase ---punctuations are preserved...sometimes punctuations are important
Language (i.e english or other languages):
	accounts for language specific stopwords and stemming and synonyms. We can mix and match different languages in same index
Some times we can make different analyzers to run on same text and store those in different fields....some tricks involved
We can for example have a english version of the same text as well as a french version as different fields.




___  ___                               _   _     _       _       _            
|  \/  |                              | | | |   (_)     | |     | |           
| .  . | ___  _ __ ___    ___  _ __   | |_| |__  _ ___  | | __ _| |_ ___ _ __ 
| |\/| |/ _ \| '__/ _ \  / _ \| '_ \  | __| '_ \| / __| | |/ _` | __/ _ \ '__|
| |  | | (_) | | |  __/ | (_) | | | | | |_| | | | \__ \ | | (_| | ||  __/ |   
\_|  |_/\___/|_|  \___|  \___/|_| |_|  \__|_| |_|_|___/ |_|\__,_|\__\___|_|   
                                                                              
                                                                              
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

-------------------------------------------------------------------------------------------------------------------

Inserting documents to an index:

The mapping for the movies index is inserted to elasticsearch as below:

In the below mapping ---movies is the name of the index and movie is the type

curl -XPUT 127.0.0.1:9200/movies -d '
{
	"mappings":{
		"movie":{
			"_all":{"enabled":false},
			"properties":{
				"year":{"type":"date"}
			}
		}
	}
}  
'

Inserting the Document :

Inserting  one single document 

curl -XPUT 127.0.0.1:9200/movies/movie/109487 -d '
{
	"genre": ["IMAX","Sci-Fi"],
	"title": "Interstellar",
	"year": 2014 
}'

The successful reponse is :
{"_index":"movies","_type":"movie","_id":"109487","_version":1,"result":"created","_shards":{"total":2,"successful":1,"failed":0},
"created":true}

How to retrive all the documents in the just created index :

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty

Response :

{
  "took" : 83,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "109487",
        "_score" : 1.0,
        "_source" : {
          "genre" : [
            "IMAX",
            "Sci-Fi"
          ],
          "title" : "Interstellar",
          "year" : 2014
        }
      }
    ]
  }
}

Inserting documents in bulk:

the multiple movie docs are in a file named : movies.json----the file contents are as below:

############################################################################

{ "create" : { "_index" : "movies", "_type" : "movie", "_id" : "135569" } }
{ "id": "135569", "title" : "Star Trek Beyond", "year":2016 , "genre":["Action", "Adventure", "Sci-Fi"] }
{ "create" : { "_index" : "movies", "_type" : "movie", "_id" : "122886" } }
{ "id": "122886", "title" : "Star Wars: Episode VII - The Force Awakens", "year":2015 , "genre":["Action", "Adventure", "Fantasy", "Sci-Fi", "IMAX"] }
{ "create" : { "_index" : "movies", "_type" : "movie", "_id" : "109487" } }
{ "id": "109487", "title" : "Interstellar", "year":2014 , "genre":["Sci-Fi", "IMAX"] }
{ "create" : { "_index" : "movies", "_type" : "movie", "_id" : "58559" } }
{ "id": "58559", "title" : "Dark Knight, The", "year":2008 , "genre":["Action", "Crime", "Drama", "IMAX"] }
{ "create" : { "_index" : "movies", "_type" : "movie", "_id" : "1924" } }
{ "id": "1924", "title" : "Plan 9 from Outer Space", "year":1959 , "genre":["Horror", "Sci-Fi"] }

############################################################################

The file contents are a little strange we have a pair of json objects for a document being indexed: 

{ "create" : { "_index" : "movies", "_type" : "movie", "_id" : "135569" } }
{ "id": "135569", "title" : "Star Trek Beyond", "year":2016 , "genre":["Action", "Adventure", "Sci-Fi"] }

The first JSON object has the name of the index, type and id of the document being indexed 
Using this information we can get a hashvalue which tells which shard the document  can be indexed to.

The second line contains the actual document to be indexed....so the elastic search will send the document to appropriate shard 
based on the hash value.

Inserting the above movies all at once:

curl -XPUT 127.0.0.1:9200/_bulk?pretty --data-binary @movies.json

Response below:

#####################################################################################


{
  "took" : 389,
  "errors" : true,
  "items" : [
    {
      "create" : {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "135569",
        "_version" : 1,
        "result" : "created",
        "_shards" : {
          "total" : 2,
          "successful" : 1,
          "failed" : 0
        },
        "created" : true,
        "status" : 201
      }
    },
    {
      "create" : {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "122886",
        "_version" : 1,
        "result" : "created",
        "_shards" : {
          "total" : 2,
          "successful" : 1,
          "failed" : 0
        },
        "created" : true,
        "status" : 201
      }
    },
    {
      "create" : {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "109487",
        "status" : 409,
        "error" : {
          "type" : "version_conflict_engine_exception",
          "reason" : "[movie][109487]: version conflict, document already exists (current version [1])",
          "index_uuid" : "BoFfCbItTRGyjJSy4TO5KQ",
          "shard" : "1",
          "index" : "movies"
        }
      }
    },
    {
      "create" : {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "58559",
        "_version" : 1,
        "result" : "created",
        "_shards" : {
          "total" : 2,
          "successful" : 1,
          "failed" : 0
        },
        "created" : true,
        "status" : 201
      }
    },
    {
      "create" : {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "1924",
        "_version" : 1,
        "result" : "created",
        "_shards" : {
          "total" : 2,
          "successful" : 1,
          "failed" : 0
        },
        "created" : true,
        "status" : 201
      }
    }
  ]
}


#####################################################################################

As we can see from the above response we are able to import movies from the movies.json

In the reponse we can see that 4 movies were imported/indexed successfully and the below document failed to be indexed

{ "id": "109487", "title" : "Interstellar", "year":2014 , "genre":["Sci-Fi", "IMAX"] }

The response that came for this document is below:
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
{
      "create" : {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "109487",
        "status" : 409,
        "error" : {
          "type" : "version_conflict_engine_exception",
          "reason" : "[movie][109487]: version conflict, document already exists (current version [1])",
          "index_uuid" : "BoFfCbItTRGyjJSy4TO5KQ",
          "shard" : "1",
          "index" : "movies"
        }
      }
    }
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

As we know we had the document for the movie "Interstellar" was already imported and hence the conflict and failure.

now to get all  indexed docs

curl -XGET 127.0.0.1:9200/movies/_search?pretty



Updateing documents:

all documents in elastic search are immutable......so how to update the documents in elasticsearch
but there is a workaround for this:

Every document which is indexed in elasticsearch will have a _version field in it
Now we can make a copy of the document which we want to update and make changes to any field in it
and increment the version field ----note that the unique id asscociated with the document and the version field 
combined togeather are unique.....this means an elasticsearch index can contain different versions of same document and
only the latest version of the document is picked up by search and all others are marked for delete and 
not considered for searching.

Retriveing an existing document using its id:

curl -XGET 127.0.0.1:9200/movies/movie/109487?pretty

response:

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

{
  "_index" : "movies",
  "_type" : "movie",
  "_id" : "109487",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "genre" : [
      "IMAX",
      "Sci-Fi"
    ],
    "title" : "Interstellar",
    "year" : 2014
  }
}


@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


As we can see from the response the _version is 1 ...ie we have the first version of the document.

Now updateing the document by changeing some field in it ....we can do it as follows:

curl -XPOST 127.0.0.1:9200/movies/movie/109487/_update -d '
{
	"doc": {
			"title":"Interstellar woo"
	}
}'
 
the response is as below:

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

{"_index":"movies","_type":"movie","_id":"109487","_version":2,"result":"updated","_shards":{"total":2,"successful":1,"failed":0}}

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

As seen from the response the version field is updated to 2   :   "_version":2

Now retriveing the document by using above curl command will get the new document version 2 .....as said before 
the old document with version 1 will not be considered for search

curl -XGET 127.0.0.1:9200/movies/movie/109487?pretty

Response:
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
{
  "_index" : "movies",
  "_type" : "movie",
  "_id" : "109487",
  "_version" : 2,
  "found" : true,
  "_source" : {
    "genre" : [
      "IMAX",
      "Sci-Fi"
    ],
    "title" : "Interstellar woo",
    "year" : 2014
  }
}
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


now inserting the same document with same ID as follows:

curl -XPUT 127.0.0.1:9200/movies/movie/109487?pretty -d '
{
	"genres":["IMAX","Sci-Fi"],
	"title":"Interstellar",
	"year": 2014
}'


The reponse is as follows:

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


{
  "_index" : "movies",
  "_type" : "movie",
  "_id" : "109487",
  "_version" : 3,
  "result" : "updated",
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "created" : false
}


@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

Now you can see eventhough we tried insering the new document with same id 
the elasticsearch updated the existing document and incremented its _version field to 3....the previous version was 2.
And also we can see that the created field is false : "created" : false ....as it updated the existing doc and not create it.

So this is how updates work....now the previous versions of the document s still exist and they will be cleaned up when 
cleanup tasks of the elasticsearch will run. 


Deleteing documents

curl -XDELETE 127.0.0.1:9200/movies/movie/58559

now while deleteing we need to get the ID of the doc we want to delete----in the above delete command 58559 is the doc being delete

Now first search for a movie that needs to be deleted----One liner search query for a Movie with Dark in its title:

curl -XGET 127.0.0.1:9200/movies/_search?q=Dark

Now theis above search query returns the document for Dark Knight as follows:

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

{"took":19,"timed_out":false,"_shards":{"total":5,"successful":5,"skipped":0,"failed":0},"hits":
{"total":1,"max_score":0.94566005,"hits":[{"_index":"movies","_type":"movie","_id":"58559","_score":0.94566005,"_source":
{ "id": "58559", "title" : "Dark Knight, The", "year":2008 , "genre":["Action", "Crime", "Drama", "IMAX"] }}]}}

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$


Using the id we can compose the delete query as below:

curl -XDELETE 127.0.0.1:9200/movies/movie/58559?pretty 

which returns the following response:
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

{
  "found" : true,
  "_index" : "movies",
  "_type" : "movie",
  "_id" : "58559",
  "_version" : 2,
  "result" : "deleted",
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  }
}

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

Now if we run the same search As below:

curl -XGET 127.0.01:9200/movies/_search?q=Dark

we get no docs related to the above search query...the response is as below:

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
{"took":13,"timed_out":false,"_shards":{"total":5,"successful":5,"skipped":0,"failed":0},"hits":
{"total":0,"max_score":null,"hits":[]}}

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

As we can see from above the hits array is empty.

Now the above delete command when run the elasticsearch marks the matching document for deletetion and the actual deletion happens 
in a saperate sweep through the index in a later phase. this is how deletion works...

in first phase -----documents are marked for delete
in second phase ----another process sweeps through the indexes and deletes all documents marked for delete

----------------------------------------------------------------------------------------------------------------------

excercise:

Insert, update, search and delete and make sure its gone

So inserting a single movie

curl -XPUT 127.0.0.1:9200/movies/movie/109559 -d '
{
	"genre":["Sci-Fi","Drama","Thirller"],
	"title": "Bicentanial Man"
	"year": 2004
}' 

Searching all documents 

curl -XGET 127.0.0.1:9200/movies/_search?pretty

the above returns all the documents

if we want to get a single doc using its id we do the following:
If we see here this is not a search ....this is like  rest call to get a resource whose id is a path param
curl -XGET 127.0.0.1:9200/movies/movie/109559?pretty
Respons e below:

{
  "_index" : "movies",
  "_type" : "movie",
  "_id" : "109559",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "genre" : [
      "Sci-Fi",
      "Drama",
      "Thirller"
    ],
    "title" : "Bicentanial Man",
    "year" : 2004
  }
}

here if we see we get the _version as 1 ----its just created

Now update the the document : here i have changed the title from "Bicentanial" to "Bicentennial"

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
curl -XPOST 127.0.0.1:9200/movies/movie/109559/_update -d '
{"doc" : {
        "title":"Bicentennial Man"
}}'


Response is as below:

{"_index":"movies","_type":"movie","_id":"109559","_version":2,"result":"updated","_shards":{"total":2,"successful":1,"failed":0}}

Note the version has changed from 1 to 2 ------
A new document is added (version 2) and the old document with version 1 has been marked for deleteion.
deletion happens in later sweep.

Note the curl commnad above is Post request and the its a rest call where we pass the "index_name/type/doc_id/_update"
Here the uri is as follows: /movies/movie/109559/_update 

and the doc we are sending in the request body is as follows:
{"doc" : {
        "title":"Bicentennial Man"
}}


The mapping for the movies index can be got as follows:

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

curl -XGET 127.0.0.1:9200/movies/_mapping/movie?pretty 
{
  "movies" : {
    "mappings" : {
      "movie" : {
        "_all" : {
          "enabled" : false
        },
        "properties" : {
          "genre" : {
            "type" : "text",
            "fields" : {
              "keyword" : {
                "type" : "keyword",
                "ignore_above" : 256
              }
            }
          },
          "genres" : {
            "type" : "text",
            "fields" : {
              "keyword" : {
                "type" : "keyword",
                "ignore_above" : 256
              }
            }
          },
          "id" : {
            "type" : "text",
            "fields" : {
              "keyword" : {
                "type" : "keyword",
                "ignore_above" : 256
              }
            }
          },
          "title" : {
            "type" : "text",
            "fields" : {
              "keyword" : {
                "type" : "keyword",
                "ignore_above" : 256
              }
            }
          },
          "year" : {
            "type" : "date"
          }
        }
      }
    }
  }
}

This is different from above when we got the mapping for the first time ......Strange-----find whats happening
Probabnly after the movie docs were indexed the mapping changed.....
note here when we created the index the mapping we created was as below....:

_____________________________________


{
	"mappings":{
		"movie":{
			"_all":{"enabled":false},
			"properties":{
				"year":{"type":"date"}
			}
		}
	}
}  

____________________________________

We created mappings for only the _all field and also the year field.....so After indexing default mappings were created for the
title and genre fields....and also note sinde genre is an array...the individual ellements are mapped as genre....which is 
awesom

But after indexing docs 

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

Now Searching the document: json_pp is a linux command for formatting json
127.0.0.1:9200/movies/_search?q=Man|json_pp

Now i am searching the index with "Man" as query string:

response as below: this returns one document with title matching Man ----Bicentennial Man
In hits json elemet total element gives num docs returned

-------------------------------------------
{
   "_shards" : {
      "skipped" : 0,
      "total" : 5,
      "successful" : 5,
      "failed" : 0
   },
   "took" : 15,
   "timed_out" : false,
   "hits" : {
      "hits" : [
         {
            "_index" : "movies",
            "_type" : "movie",
            "_source" : {
               "year" : 2004,
               "genre" : [
                  "Sci-Fi",
                  "Drama",
                  "Thirller"
               ],
               "title" : "Bicentennial Man"
            },
            "_id" : "109559",
            "_score" : 0.7787034
         }
      ],
      "total" : 1,
      "max_score" : 0.7787034
   }
}

-------------------------------------------
Now searching on genres which was indexed as an array :

curl -XGET 127.0.0.1:9200/movies/_search?q=Sci-Fi|json_pp

response below: returned 5 results or docs

--------------------------------------------

{
   "took" : 76,
   "hits" : {
      "total" : 5,
      "max_score" : 1.1821114,
      "hits" : [
         {
            "_index" : "movies",
            "_source" : {
               "genre" : [
                  "Action",
                  "Adventure",
                  "Fantasy",
                  "Sci-Fi",
                  "IMAX"
               ],
               "title" : "Star Wars: Episode VII - The Force Awakens",
               "id" : "122886",
               "year" : 2015
            },
            "_score" : 1.1821114,
            "_type" : "movie",
            "_id" : "122886"
         },
         {
            "_index" : "movies",
            "_score" : 0.5753642,
            "_source" : {
               "year" : 2016,
               "id" : "135569",
               "genre" : [
                  "Action",
                  "Adventure",
                  "Sci-Fi"
               ],
               "title" : "Star Trek Beyond"
            },
            "_type" : "movie",
            "_id" : "135569"
         },
         {
            "_score" : 0.5063205,
            "_source" : {
               "year" : 2014,
               "title" : "Interstellar",
               "genres" : [
                  "IMAX",
                  "Sci-Fi"
               ]
            },
            "_index" : "movies",
            "_id" : "109487",
            "_type" : "movie"
         },
         {
            "_type" : "movie",
            "_id" : "1924",
            "_index" : "movies",
            "_source" : {
               "genre" : [
                  "Horror",
                  "Sci-Fi"
               ],
               "title" : "Plan 9 from Outer Space",
               "id" : "1924",
               "year" : 1959
            },
            "_score" : 0.34450945
         },
         {
            "_id" : "109559",
            "_type" : "movie",
            "_source" : {
               "genre" : [
                  "Sci-Fi",
                  "Drama",
                  "Thirller"
               ],
               "title" : "Bicentennial Man",
               "year" : 2004
            },
            "_score" : 0.34450945,
            "_index" : "movies"
         }
      ]
   },
   "timed_out" : false,
   "_shards" : {
      "successful" : 5,
      "failed" : 0,
      "skipped" : 0,
      "total" : 5
   }
}

--------------------------------------------



$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

Now deleteing

curl -XDELETE 127.0.0.1:9200/movies/movie/109559?pretty
response:
{
  "found" : true,
  "_index" : "movies",
  "_type" : "movie",
  "_id" : "109559",
  "_version" : 3,
  "result" : "deleted",
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  }
}

now tryng to find the doc using the rest service as below:

curl -XGET 127.0.0.1:9200/movies/movie/109559
{"_index":"movies","_type":"movie","_id":"109559","found":false}

So now the document is marked for deletion and deleteion will itself happen in another sweep through the index.





-------------------------------------------------------------------------------------------------------------------

Concurrency in Elastic search:

Optimistic locks using version fields of document----same as optimistic locking in hibernate

And use of retry_on_conflict request parm while updateing the document.....the elstic search will get the document and 
update the field value and try submitting the document ....but if the version is different it fails....the presence of this parameter in the
update request will start a retry loop...which will do retry update the number of times specified as the value of this param

TODO More on this later

The version update is as follows:

So assume that we got the document with version 3 as follows:

curl -XGET 127.0.0.1:9200/movies/movie/109487?pretty
{
  "_index" : "movies",
  "_type" : "movie",
  "_id" : "109487",
  "_version" : 3,
  "found" : true,
  "_source" : {
    "genres" : [
      "IMAX",
      "Sci-Fi"
    ],
    "title" : "Interstellar",
    "year" : 2014
  }
}

Now we can see we have version 3 of the doc

Now updateing the same using the version is as follows:
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

curl -XPUT 127.0.0.1:9200/movies/movie/109487?version=3 -d '
{
   "genres" : [
      "IMAX",
      "Sci-Fi"
    ],
    "title" : "Interstellar Foo",
    "year" : 2014

}'

Response below:

{"_index":"movies","_type":"movie","_id":"109487","_version":4,"result":"updated","_shards":
{"total":2,"successful":1,"failed":0},"created":false}

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

Now this will change the version to 4 as seen from above response:

curl -XGET 127.0.0.1:9200/movies/movie/109487?pretty
{
  "_index" : "movies",
  "_type" : "movie",
  "_id" : "109487",
  "_version" : 4,
  "found" : true,
  "_source" : {
    "genres" : [
      "IMAX",
      "Sci-Fi"
    ],
    "title" : "Interstellar Foo",
    "year" : 2014
  }
}

Now if we try using version 3 to update same doc using same curl command above we will get exception as below:

?????????????????????????????????????????????????

{"error":{"root_cause":[{"type":"version_conflict_engine_exception","reason":
"[movie][109487]: version conflict, current version [4] is different than the one provided [3]",
"index_uuid":"BoFfCbItTRGyjJSy4TO5KQ","shard":"1","index":"movies"}],"type":"version_conflict_engine_exception",
"reason":"[movie][109487]: version conflict, current version [4] is different than the one provided [3]","index_uuid":
"BoFfCbItTRGyjJSy4TO5KQ","shard":"1","index":"movies"},"status":409}

??????????????????????????????????????????????????
This is how optimistic locking works and we can have a workaround for this problem if we wnat updates to go 
through even if we have conflicts we can run the following curl command with an extra parameter retry_on_conflicts=n 
where n is the number of retries:

Note the difference here we are using a _update rest service...which under the hood retrives the existing document (current version),
Change it and submit a new document .....this is effectively doing the things we did before with version update above.

Additionaly if the update fails due to version conflit...it does the whole thing of retriveing the current version of the doc,
update it and submit a new document....version changes will be done when the doc gets saved by the elasticsearch.
 

curl -XPOST 127.0.0.1:9200/movies/movie/109559/_update?retry_on_conflict=5 -d '
{"doc" : {
        "title":"Bicentennial Man"
}}'

*****************And also note this is POST request not the put request as we have done before with version param******************








---------------------------------------------------------------------------------------------------------------

Using analysers and tokenizers -- Controlling full text search:

We have seen in brief how Analyzers control full text search in our docs. now more details:
#################################
If we need the text fields to matched exactly -----dont use analyzer use "no_analyzer" mapping when we are mapping the field
#################################
By default fields have a default analyzer which will break things up into individual search terms ----and if we do a 
search with any of the search terms ....docs with these terms will be returned as part of search result...we can sort the results
by relevance but the results may not be what the user wants.

Search on analyzed fields will return anything remotely relevant:
	1. Depending upon the analyzer chosen, results will be case insensitive, stemmed, stop words removed, synonyms applied etc
	2. searches with multiple terms may not match them all....the search results may have matched only a part of the phrase

we can sort the results by relevance but the results might be unexpected

Now lets see how full text works and how we can modify the mappings to get the results that we really want....
Note that the mappings should reflect the results that we expect from the full text search. ----we need to think about our 
query patterns upfront. 

Now for some examples:

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
	"query":{
		"match":{
			"title":"Star Trek"
		}
	}
}'

Response is as below:

#####################################################

{
  "took" : 19,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 0.70853925,
    "hits" : [
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "122886",
        "_score" : 0.70853925,
        "_source" : {
          "id" : "122886",
          "title" : "Star Wars: Episode VII - The Force Awakens",
          "year" : 2015,
          "genre" : [
            "Action",
            "Adventure",
            "Fantasy",
            "Sci-Fi",
            "IMAX"
          ]
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "135569",
        "_score" : 0.5063205,
        "_source" : {
          "id" : "135569",
          "title" : "Star Trek Beyond",
          "year" : 2016,
          "genre" : [
            "Action",
            "Adventure",
            "Sci-Fi"
          ]
        }
      }
    ]
  }
}

#####################################################

This above response cleary needs explaination as we can See we searched for the phrase "Star Trek" but in reponse
we get "Star Wars" having a higher relevance score than the Exact match ie.."Star Trek Beyond". this is sort of strange.
Why did elasticsearch associated "Star Wars" higher relevance than the "Star Trek"---the phrase we searched for

This is an anomly and can be explianed as follows: 
This can happen when have too many shards and not enough documents things dont work as expected.

The problem is that Inverse docuement frequency is only computed per shard
Now the problem is Star Trek and Star wars are on different shards and they both have inverse document frequency scores for the 
term star which ended up giving higher boost in one shard----they would have been equal if they were on a single shard.

This issue wont happen if we have a large corpus of documents indexed into elastic search.------no worries just a anomaly.

If the the documents are very less its better to have them all in asingle shard.


Aprt from the above anomly we have one more thing to explain ie...even though we searched for "Star Terk" we got docs containing "Star Wars"
---We can say we got all docs containing Star in them....This is how matches on analyzed fields work ---they try to give us anything that remotely matches our search ----it gives docs in relevance order and some times the elastic search gets it right and some other 
times as in our case it may go wrong as well. 

If we are ok with this kind of behaviour then its fine ....if not we need to think about how we structure our queries and
how we analyse data that we want to index.

Now another example is ---Pharse search ----

Example is searching for the Phrase "Sci" ----this is a phrase search....lets see what happens:

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
	"query":{
		"match_phrase":{
				"genre": "Sci"
		}
	}
}'

Response Below:

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

{
  "took" : 6,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 3,
    "max_score" : 0.5910557,
    "hits" : [
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "122886",
        "_score" : 0.5910557,
        "_source" : {
          "id" : "122886",
          "title" : "Star Wars: Episode VII - The Force Awakens",
          "year" : 2015,
          "genre" : [
            "Action",
            "Adventure",
            "Fantasy",
            "Sci-Fi",
            "IMAX"
          ]
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "135569",
        "_score" : 0.2876821,
        "_source" : {
          "id" : "135569",
          "title" : "Star Trek Beyond",
          "year" : 2016,
          "genre" : [
            "Action",
            "Adventure",
            "Sci-Fi"
          ]
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "1924",
        "_score" : 0.25316024,
        "_source" : {
          "id" : "1924",
          "title" : "Plan 9 from Outer Space",
          "year" : 1959,
          "genre" : [
            "Horror",
            "Sci-Fi"
          ]
        }
      }
    ]
  }
}

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

Again this Search result needs lot of explaination: Here we queried for the Phrase "Sci" 
but we got all documents having genre "Sci-Fi" how did this happen??????

This is because we have default analyser for the genre field.
The default analyzer has split the genre into individual terms and indexed them.
So for example here Sci-Fi has been split into phrases "Sci" and "Fi"

So as stated before ...if we want exact match we need to disable analyzers on such fields as below:

Now we cant modify an index we will delete the entire index and create a new one with modified mapping:

Deleteing the entire index:

curl -XDELETE 127.0.0.1:9200/movies

Response below:
{"acknowledged":true}

Now the new mapping as below:

curl -XPUT 127.0.0.1:9200/movies -d'
{
	"mappings": {
		"movie": {
			"_all": {"enabled": false},
			"properties": {
				"id": {"type": "integer"},
				"year": {"type":"date"},
				"genre": {"type":"string","index":"not_analyzed"},
				"title": {"type": "string", "analyzer":"english"}
			}		
		}
	}
}'

========================================================================================
The mapping types have changed in elasticsearch 6.0
the string type is deprecated and instead we have a type called text.
and instead of qualifying the "index": "not_analyzed" we now have a type called keyword as below:

curl -XPUT 127.0.0.1:9200/movies -H "content-type: application/json" -d'
{
        "mappings": {
                "movie": {
                        "_all": {"enabled": false},
                        "properties": {
                                "id": {"type": "integer"},
                                "year": {"type": "date"},
                                "genre": {"type": "keyword"},
                                "title": {"type": "text","analyzer": "english"}
                        }
                }
        }
}'

=========================================================================================	

Response below:

{"acknowledged":true,"shards_acknowledged":true,"index":"movies"}

Now observe the mapping here for genre its mapped as  "genre": {"type":"string","index":"not_analyzed"}
and title is as follows: "title": {"type": "string", "analyzer":"english"}

Here genre is indexed but not analyzed ie...not broken into terms ---indexed as it is
and title is analyzed...accounts for language specific stopwords and stemming and synonyms
Even though some titles have foreign language this is chosen....lets see what happens 


Now the below search query:

############################################################

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
	"query":{
		"match_phrase":{
				"genre": "Sci"
		}
	}
}'
 
this returns no results at all:

Response below:

{
  "took" : 4,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}

reason being the genere is mapped as not_analyzed....its not broken down into terms ie..sci and fi

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
	"query":{
		"match":{
				"genre": "Sci"
		}
	}
}'

Neither the above works....since this field is not analyzed even if we do not provice proper casing it does not work
This is the typical behaviour of a non analyzed field

Below works and gets the result:

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
	"query":{
		"match":{
				"genre": "Sci-Fi"
		}
	}
}'
############################################################

Now lets search for analyzed field like title:

Below is the query and the result:

###########################################################
curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
"query":{
"match":{
"title":"star trek"
}
}
}'
{
  "took" : 9,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 0.6284925,
    "hits" : [
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "122886",
        "_score" : 0.6284925,
        "_source" : {
          "id" : "122886",
          "title" : "Star Wars: Episode VII - The Force Awakens",
          "year" : 2015,
          "genre" : [
            "Action",
            "Adventure",
            "Fantasy",
            "Sci-Fi",
            "IMAX"
          ]
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "135569",
        "_score" : 0.5063205,
        "_source" : {
          "id" : "135569",
          "title" : "Star Trek Beyond",
          "year" : 2016,
          "genre" : [
            "Action",
            "Adventure",
            "Sci-Fi"
          ]
        }
      }
    ]
  }
}

Here we can see for an analyzed field the results indicate that anything that remotely matches the search will be returned
###########################################################

TF - IDF (term frequency inverse document frequency):

By default, the algorithm used to calculate a document’s relevancy score is TF-IDF .
TF-IDF stands for term frequency–inverse docu-
ment frequency, which are the two factors that influence relevancy score.

1.Term frequency—The more times the words you’re looking for appear in a docu-
ment, the higher the score.

2.Inverse document frequency—The weight of each word is higher if the word is
uncommon across other documents.

For example, if you’re looking for “bicycle race” on a cyclist’s blog, the word “bicycle”
counts much less for the score than “race.” But the more times both words appear in a
document, the higher that document’s score.

#######################################################################################


Datamodelling in elasticsearch

----Denormalize the data ----which is true for Cassandra, mongodb and other nosql stores. Relational db's store data in a normalized way.

Elastic search provides facilities for normalizing as well and also can represent parent child relationships.
In case of normailizing the data we might need to do 2 different queries for getting the complete data.
As an example:

We can lookup for a rating -----which has Type: Rating:{userid,movieid, rating}

But in a seperate query we have to get the movie title from the type movie:{movieid,title,genre}

the data that is normalized is not redundent and its easy to change....but it needs two queries to fetch it.
So if we normalize we would be doubling the search trafic. And since in most of the websites search performace is what matters
and the storage is cheap so we need worry about the storge costs as much we worry about a performant website.

Now the denormalized data for the ratings type may look like follows:

Type Rateing:{userid,movieid,title,rateing} so this would help us lookuo the movie ratings in a single query
Only issue is that since the movie name is repeated accross multiple ratings docs now....if the movie name changes then we
have to update all rating docs for that movie.....may not happen as movie titles seldom change. Storage is not an issue.

Manageing the parent child relationship in elastic search:

Lets say we want to represent the relationship between the movie franchises and movies that makeup the the franchise

like Star wars and the star wars franchises {A new hope, Empire strikes back, Return of the jedi, the force awakens}

So the next mapping sets up such a parent child relation between the movie and the its parent ie..franchise

curl -XPUT 127.0.0.1:9200/series -d'
 {
 "mappings": {
       "franchise": {},
       "film": {
               "_parent": {
                       "type": "franchise"
               }
       }
 }
 }'

downoad series data :
wget http://media.sundog-soft.com/es/series.json

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

mapping for the get-together index and index type ----here the event is a child for the group type:
curl -XPUT 127.0.0.1:9200/get-together -d '
{
    "mappings" : {
      "event" : {
	"_parent": {
                       "type": "group"
         },
        "properties" : {
          "attendees" : {
            "type" : "text",
            "fields" : {
              "keyword" : {
                "type" : "keyword",
                "ignore_above" : 256
              }
            }
          },
          "date" : {
            "type" : "date"
          },
          "description" : {
            "type" : "text",
            "fields" : {
              "keyword" : {
                "type" : "keyword",
                "ignore_above" : 256
              }
            }
          },
          "host" : {
            "type" : "text",
            "fields" : {
              "keyword" : {
                "type" : "keyword",
                "ignore_above" : 256
              }
            }
          },
          "location_event" : {
            "properties" : {
              "geolocation" : {
                "type" : "text",
                "fields" : {
                  "keyword" : {
                    "type" : "keyword",
                    "ignore_above" : 256
                  }
                }
              },
              "name" : {
                "type" : "text",
                "fields" : {
                  "keyword" : {
                    "type" : "keyword",
                    "ignore_above" : 256
                  }
                }
              }
            }
          },
          "reviews" : {
            "type" : "long"
          },
          "title" : {
            "type" : "text",
            "fields" : {
              "keyword" : {
                "type" : "keyword",
                "ignore_above" : 256
              }
            }
          }
        }
      }
    }
  }'


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
import the data : index into elastic search:

curl -XPUT 127.0.0.1:9200/_bulk?pretty --data-binary @series.json

###########################################################

{
  "took" : 378,
  "errors" : false,
  "items" : [
    {
      "create" : {
        "_index" : "series",
        "_type" : "franchise",
        "_id" : "1",
        "_version" : 1,
        "result" : "created",
        "_shards" : {
          "total" : 2,
          "successful" : 1,
          "failed" : 0
        },
        "created" : true,
        "status" : 201
      }
    },
    {
      "create" : {
        "_index" : "series",
        "_type" : "film",
        "_id" : "260",
        "_version" : 1,
        "result" : "created",
        "_shards" : {
          "total" : 2,
          "successful" : 1,
          "failed" : 0
        },
        "created" : true,
        "status" : 201
      }
    },	

..........
Now if we see the above fragment of response we can see we imported the franchise type first and then the 
series.
###########################################################

Now query to get all movies in "Star Wars" franchise

-----------------------------------------------------------------------------
curl -XGET 127.0.0.1:9200/series/film/_search?pretty -d'
> {
> "query": {
>       "has_parent": {
>               "type": "franchise",
>               "query" : {
>                       "match": {
>                               "title": "Star Wars"
>                       }
>               }
>       }
> }
> }'

response below:

{
  "took" : 201,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 7,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "series",
        "_type" : "film",
        "_id" : "260",
        "_score" : 1.0,
        "_routing" : "1",
        "_parent" : "1",
        "_source" : {
          "id" : "260",
          "title" : "Star Wars: Episode IV - A New Hope",
          "year" : "1977",
          "genre" : [
            "Action",
            "Adventure",
            "Sci-Fi"
          ]
        }
      },
      {
        "_index" : "series",
        "_type" : "film",
        "_id" : "1196",
        "_score" : 1.0,
        "_routing" : "1",
        "_parent" : "1",
        "_source" : {
          "id" : "1196",
          "title" : "Star Wars: Episode V - The Empire Strikes Back",
          "year" : "1980",
          "genre" : [
            "Action",
            "Adventure",
            "Sci-Fi"
          ]
        }
      },
      {
        "_index" : "series",
        "_type" : "film",
        "_id" : "1210",
        "_score" : 1.0,
        "_routing" : "1",
        "_parent" : "1",
        "_source" : {
          "id" : "1210",
          "title" : "Star Wars: Episode VI - Return of the Jedi",
          "year" : "1983",
          "genre" : [
            "Action",
            "Adventure",
            "Sci-Fi"
          ]
        }
      },
      {
        "_index" : "series",
        "_type" : "film",
        "_id" : "2628",
        "_score" : 1.0,
        "_routing" : "1",
        "_parent" : "1",
        "_source" : {
          "id" : "2628",
          "title" : "Star Wars: Episode I - The Phantom Menace",
          "year" : "1999",
          "genre" : [
            "Action",
            "Adventure",
            "Sci-Fi"
          ]
        }
      },
      {
        "_index" : "series",
        "_type" : "film",
        "_id" : "5378",
        "_score" : 1.0,
        "_routing" : "1",
        "_parent" : "1",
        "_source" : {
          "id" : "5378",
          "title" : "Star Wars: Episode II - Attack of the Clones",
          "year" : "2002",
          "genre" : [
            "Action",
            "Adventure",
            "Sci-Fi",
            "IMAX"
          ]
        }
      },
      {
        "_index" : "series",
        "_type" : "film",
        "_id" : "33493",
        "_score" : 1.0,
        "_routing" : "1",
        "_parent" : "1",
        "_source" : {
          "id" : "33493",
          "title" : "Star Wars: Episode III - Revenge of the Sith",
          "year" : "2005",
          "genre" : [
            "Action",
            "Adventure",
            "Sci-Fi"
          ]
        }
      },
      {
        "_index" : "series",
        "_type" : "film",
        "_id" : "122886",
        "_score" : 1.0,
        "_routing" : "1",
        "_parent" : "1",
        "_source" : {
          "id" : "122886",
          "title" : "Star Wars: Episode VII - The Force Awakens",
          "year" : "2015",
          "genre" : [
            "Action",
            "Adventure",
            "Fantasy",
            "Sci-Fi",
            "IMAX"
          ]
        }
      }
    ]
  }
}

--------------------------------------------------------------------------------------------

now getting the franchise given one of its constituent film:

curl -XGET 127.0.0.1:9200/series/franchise/_search?pretty -d'
{
	"query":{
		"has_child":{
			"type": "film",
			"query":{
				"match":{
					"title" : "The Force Awakens"
				}
			}
		}
	}
}'

response below:

{
  "took" : 13,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "series",
        "_type" : "franchise",
        "_id" : "1",
        "_score" : 1.0,
        "_source" : {
          "id" : "1",
          "title" : "Star Wars"
        }
      }
    ]
  }
}
--------------------------------------------------------------------------------------

################################################################################
Querying with Elastic search---------------This is really important

Query Lite ---this is the shortcut for querying in elasticsearch

No request body 

queries are of the form:

/movies/movie/_search?q=title:Dark

This below query will not work as it is ...the request parameters "q=+year:>2010+title:trek" need url encodeing
So the readability of such queries will loose radability.

/movies/movie/_search?q=+year:>2010+title:trek

The shorthand technique is dangerous and not be used in prod because of following reasons:

1. The short-hand queries can get pretty cryptic and will be difficult to debug.
2. Can be security issue if exposed to end users....end users can issue a intensive query that can bring down the cluster.
3. fragile....one wrong character and the query will fail.

Handy for quick experimentation.

The query-lite is formally known as URI search
to know more go to

https://www.elastic.co/guide/en/elasticsearch/reference/current/search-uri-request.html

TODO go through the stuff in the above URL

Now for some examples:

Searching for docs with title containing star in them:

-----------------------------------------------------------------------------

curl -XGET "127.0.0.1:9200/movies/movie/_search?q=title:star&pretty"

response below:

{
  "took" : 8,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 0.6284925,
    "hits" : [
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "122886",
        "_score" : 0.6284925,
        "_source" : {
          "id" : "122886",
          "title" : "Star Wars: Episode VII - The Force Awakens",
          "year" : 2015,
          "genre" : [
            "Action",
            "Adventure",
            "Fantasy",
            "Sci-Fi",
            "IMAX"
          ]
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "135569",
        "_score" : 0.25316024,
        "_source" : {
          "id" : "135569",
          "title" : "Star Trek Beyond",
          "year" : 2016,
          "genre" : [
            "Action",
            "Adventure",
            "Sci-Fi"
          ]
        }
      }
    ]
  }
}

-----------------------------------------------------------------------------

the next example shows how the URI search can return really cryptic results and makes it really difficult to debug:

curl -XGET "127.0.0.1:9200/movies/movie/_search?q=+year:>2010+title:trek&pretty"
--------------------------------------------------------------


{
  "took" : 39,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 3,
    "max_score" : 1.2531602,
    "hits" : [
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "135569",
        "_score" : 1.2531602,
        "_source" : {
          "id" : "135569",
          "title" : "Star Trek Beyond",
          "year" : 2016,
          "genre" : [
            "Action",
            "Adventure",
            "Sci-Fi"
          ]
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "122886",
        "_score" : 1.0,
        "_source" : {
          "id" : "122886",
          "title" : "Star Wars: Episode VII - The Force Awakens",
          "year" : 2015,
          "genre" : [
            "Action",
            "Adventure",
            "Fantasy",
            "Sci-Fi",
            "IMAX"
          ]
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "109487",
        "_score" : 1.0,
        "_source" : {
          "id" : "109487",
          "title" : "Interstellar",
          "year" : 2014,
          "genre" : [
            "Sci-Fi",
            "IMAX"
          ]
        }
      }
    ]
  }
}

As we can see from response we did not get what we expected ....we expected any movie whose title has "trek"
and also the year must be grater than 2010.

But the result returned wrong rows ....this is because the query parameters needed url encodeing as it contins special characters like +.
This is the reason why we should not use the URL queries as it can get really confusing for complex queries.

if we url encode the above query params we can get the results as expected

The actual search query and URL encoded query string is as as below:
curl -XGET "127.0.0.1:9200/movies/movie/_search?q=+year:>2010+title:trek&pretty"

curl  -XGET "127.0.0.1:9200/movies/movie/_search?q=%2Byear%3A%3E2010+%2Btitle%3Atrek&pretty"

As we can see the quey becomes a pain in the ass

in the below i have tried to url encode the query string but it does  -----url encodeing in curl

curl -G -v -XGET "127.0.0.1:9200/movies/movie/_search" --data-urlencode "q=+year:>2010+title:trek&pretty"

--------------------------------------------------------------


Request body search:

More structured definition for the search query ---its a JSON DSL



Thr URI search query : curl -XGET "127.0.0.1:9200/movies/movie/_search?q=title:star&pretty"

equivallent to the above query in request body search is as follows:

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
	"query": {
		"match":{
			"title":"star"
		}
	}
}'

One more strange thing we note here is that the GET request having a body:
GET requests can have bodies........amazing!!!!!!

Request body queries can have 2 different things in them

1. Filters : Ask yes/no question of our data


2. Queries : return data in terms of relevance 
Queries are used for example when we search for a search term like "trek" the query will find out the relevance of the term "trek"
to that document and order in terms of its relevance.

However if we have a binary yes/no we need a filter...use filters when you can they are faster and cacheable

Example of boolean query with filter----equvallent to the URI search: 
curl -XGET "127.0.0.1:9200/movies/movie/_search?q=+year:>2010+title:trek&pretty"

Below the request body query:

This query , queries for term trek in all docs which have year field greater than 2010-----query

must is the query and filter is filtering ie...(boolean operation on the year field)-----filter 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Queries are nested in query block and filters are nested in filter block

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
################################################################################

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
	"query": {
		"bool": {
			"must":{"term": {"title": "trek"}},
			"filter": {"range": {"year":{"gte":2010}}}
		}
	}
}		

Response is as below:

{
  "took" : 14,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.25316024,
    "hits" : [
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "135569",
        "_score" : 0.25316024,
        "_source" : {
          "id" : "135569",
          "title" : "Star Trek Beyond",
          "year" : 2016,
          "genre" : [
            "Action",
            "Adventure",
            "Sci-Fi"
          ]
        }
      }
    ]
  }
}

################################################################################

Some of the types of filters:

term: filter by exact values {"term": {"year":2014}}

terms: filter if any exact value in a list match  {terms:{"genre":["Sci-Fi","Drama"]}}

range: Find numbers or dates in a given range (lt,gt,gte,lte) {"range":{"year":{"gte":2010}}}

exists: Find documents where field exists {"exists": {"field":"tags"}}

missing: Find documents where field is missing {"missing": {"field":"tags"}}

exists and missing are filters necessary as document may or may not have certian fields at all------loose schema/loosely binding

bool: Combine filters with boolean logic (must, must_not, should)

Example of bool: 
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
"bool": {
			"must":{"term": {"title": "trek"}},
			"filter": {"range": {"year":{"gte":2010}}}
		}

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
in the above we have term filter and range filters in action

Some types of queries:

match_all: returns all documents ...its the default .usully used with a filter ----{"match_all":{}} 
this is implicit if we do not have any query.

match: searches anylyzed results, such as full text search
{"match":{"title":"star"}} ----relevent search for search term or terms

multy_match: query multile fields at same time. ie..run the same quey on multiple fields...Eg
{"multi_match": {"query": "star","fields":["title","synopsis"]}}

query for term star in fields title and synopsis.

bool: works like bool filter , but results are scored by relevance.

Syntax reminder:

queries are wrapped in a query block

Filters are wrapped in filter block

We can combine filters inside query blocks and queries inside filter blocks too
Example:

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
	"query": {
		"bool": {
			"must":{"term": {"title": "trek"}},
			"filter": {"range": {"year":{"gte":2010}}}
		}
	}
}

Some examples of doing complex queries with query and filter blocks:

Simple query example:

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
	"query": {
		"match": {
			"title": "star"
		}
	}
}'

Another query for querying trek term in title and the year greater than 2010:

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d '
{
	"query": {
		"bool": {
			"must": {"term": {"title": "trek"}},
			"filter": {"range": {"year": {"gte": "2010"}}}
		}
	}
}'
 
------------------------------------------------------------------------------------------

Phrase search:

Must Find all terms in same order: ---very important
Eg:

##############################################################################
curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d '
{
	"query": {
		"match_phrase": {
			"title": "star wars"
		}
	}
}'

response below:

{
  "took" : 212,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.256985,
    "hits" : [
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "122886",
        "_score" : 1.256985,
        "_source" : {
          "id" : "122886",
          "title" : "Star Wars: Episode VII - The Force Awakens",
          "year" : 2015,
          "genre" : [
            "Action",
            "Adventure",
            "Fantasy",
            "Sci-Fi",
            "IMAX"
          ]
        }
      }
    ]
  }
}

############################################################################

we are searching for the phrase "star wars" in title.

The way this works in the inverted index .....it not only stores the sarch terms which occour in the document but also stores the order in
which those terms occour---

Inverted Index stores the search terms in documents as well as the order in which those terms occour in the document.----very very important

So elastic search uses this information about the order in which the search term occours to peice togeather the phrase searches.

Slop:

Order matters but we can also search for a phrase with some words between the words of that given phrase being searched.

Eg:
###################################################################
curl -XGET 192.0.0.1:9200/movies/movie/_search -d'
{
	"query": {
		"match_phrase": {
			"title": {"query": "star beyond","slop": 1}
		}
	}
}'

response :

{
  "took" : 14,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.3164503,
    "hits" : [
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "135569",
        "_score" : 0.3164503,
        "_source" : {
          "id" : "135569",
          "title" : "Star Trek Beyond",
          "year" : 2016,
          "genre" : [
            "Action",
            "Adventure",
            "Sci-Fi"
          ]
        }
      }
    ]
  }
}
###################################################################

Notice the difference with the above query!!!!!!!


Slop represents how far we are willing to let a term move to satisfy a phrase ----in either direction

Proximity queries:

Use really high slop value if we want any document which contain the words in the given search phrase, but want 
the documents that have the words closer together scored higher.

remeber this is aquery and - results are sorted by relevance.


curl -XGET 127.0.0.1:9200/movies/movie_search?pretty -d'
{
	"query": {
		"match_phrase": {
			"title": {"query": "star beyond", "slop": 100}
		}
	}
}'

TODO more on slops later ----find what is meant by both ways?????????????????????????? 

-----------------------------------------------------------------------------------

Now the difference between simple match query and the match_phrase query:

match query:

#############################################################################
curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d '
{
	"query": {
		"match": {
			"title": "star trek"
		}
	}
}'
 
response:

{
  "took" : 17,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.256985,
    "hits" : [
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "122886",
        "_score" : 1.256985,
        "_source" : {
          "id" : "122886",
          "title" : "Star Wars: Episode VII - The Force Awakens",
          "year" : 2015,
          "genre" : [
            "Action",
            "Adventure",
            "Fantasy",
            "Sci-Fi",
            "IMAX"
          ]
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "135569",
        "_score" : 0.25316024,
        "_source" : {
          "id" : "135569",
          "title" : "Star Trek Beyond",
          "year" : 2016,
          "genre" : [
            "Action",
            "Adventure",
            "Sci-Fi"
          ]
        }
      }
    ]
  }
}

Notice in this query the search terms are broken down and any doc which contins the any of the terms will be picked up
The doc with maximum of search terms will be ranked higher in the results. ----higher relevancy score

#############################################################################	

So if we want only the docs which have the given phrase and in that order eg: "star wars" what we want is a phrase search

#############################################################################

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d '
{
"query": {
        "match_phrase": {
                "title": "star wars"
        }
}
}'

The above query gives us only docs which have "star wars" phrase in their title

############################################################################

Excercise : search for Star Wars movie released after 1980 using both URI and request body search
 

curl -XGET "127.0.0.1:9200/movies/movie/_search?q=+year:>1980+title:star wars&pretty"

curl  -XGET "127.0.0.1:9200/movies/movie/_search?q=%2Byear%3A%3E1980+%2Btitle%3A(star+wars)&pretty"

This above query gives "star wars" and "star trek" since we are not doing a phrase search	

This below query gives the only doc which has "star wars" phrase as we are doing the phrase search (match_phrase)
curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d '
{
        "query": {
                "bool":{
                        "must": {"match_phrase": {"title": "star wars"}},
                        "filter": {"range": {"year": {"gte":1980}}}
                }
        }
}'

Reaponse: 

{
  "took" : 8,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.256985,
    "hits" : [
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "122886",
        "_score" : 1.256985,
        "_source" : {
          "id" : "122886",
          "title" : "Star Wars: Episode VII - The Force Awakens",
          "year" : 2015,
          "genre" : [
            "Action",
            "Adventure",
            "Fantasy",
            "Sci-Fi",
            "IMAX"
          ]
        }
      }
    ]
  }
}
--------------------------------------------------------------------------------------

Pagination:

There are 2 request parms which we can pass either in query string or in JSON body:

from and size from ----from points to current postion of cursor ------starts from 0
And the size is the number of documents that need to be returned.

examples are follows:

curl -XGET "127.0.0.1:9200/movies/movie/_search?from=0&size=3&pretty"

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
	
		"from": 0,
		"size": 2,
		"query": {"match": {"title": "star"}}
}'

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d '
{
        "from": 0,
        "size": 3,
        "query": {"match": {"genre": "Sci-Fi"}}
}
'

Beware:

Deep paging will kill performance.
Like if we ask for documents starting from 100003 and size being 10 .....even though we are asking for 10 docs
elasticsearch will have to retrive all documents sort them in relevance order and then get the documents from
specified postion.

every result must be retrived, collected and sorted.

enforce upperbounds on how many results are returned to users----else some malicious users can manipulate
the bounds and bring the search down.

Examples:

curl -XGET "127.0.0.1:9200/movies/movie/_search?size=2&pretty"

in the above query from by default is 0.

next 2 results are got by :

curl -XGET "127.0.0.1:9200/movies/movie/_search?size=2&from=2&pretty"

from is 0 based and in the first query we got results are of index 0 and 1. in the next query is getting indexes 2 and 3


--------------------------------------------------------------------------------------------
Sorting search results:

Sorting is easy ....but it can complicate matters in some situations:

Sorting can be done on a field by providing the name of the field on which the results are sorted:

curl -XGET "127.0.0.1:9200/movies/movie/_search?sort=year&pretty"

gives all docs sorted in the ascending order of year

This simple but other cases like:

Sorting on fields that are analyzed:

A string field that is analyzed for full text search cannot be used to sort search results.
because the field exists in inverted index as individual terms, not as the entire string.

Now if we want to sort on analyzed field we need to drop the existing index and create
a new mapping for keeping a non analyzed copy of the field

So we will have 2 fields instead of one ----
one is analyzed and enabled for full text search
other is non ananlyzed and can be used for sorting purposes:

The point is we need to understand how a field will be used in full text search before 
creating mappings or else we may have to delete and reindex again.-----painful in some scenarios

The new mapping for movie is as below----we will delete the existing and add new mpping
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
curl -XPUT 127.0.0.1:9200/movies -d'
{
	"mappings":{
		"movie":{
			"_all": {"enabled":false},
			"properties": {
				"title": {
					"type": "string",
					"fields": {
						"raw":{
							"type": "String",
							"index": "not_analyzed"
						}
					}
				}
			}
		}
	}
}'


In this mapping the title field has a sub field title.raw

So we now have title which has a default analyzer associated with it ...which will break the text in the field to
terms and index the same.

Additionally we have a title.raw field which will not be analyzed and can be used for sorting.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

remember this trick ie.. keeping a raw version of a field along with analyzed version will help in lot of other 
situations.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$


Now once we have this mapping in and reindexed all the docs we can sort using the title.raw field as 
follows:

curl -XGET "127.0.0.1:9200/movies/movie/_search?sort=title.raw&pretty"

So this is possible only after deleting existing index and reindex the docs 
it is not possible to  change the mapping of an existing index.
Just like number of shards ...we will have to think about this before indexing.

In our existing index where we do not have raw field for title if we try srting by title:

curl -XGET "127.0.0.1:9200/movies/movie/_search?sort=title&pretty"

We get exception:

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

{
  "error" : {
    "root_cause" : [
      {
        "type" : "illegal_argument_exception",
        "reason" : "Fielddata is disabled on text fields by default. Set fielddata=true on [title] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead."
      }
    ],
    "type" : "search_phase_execution_exception",
    "reason" : "all shards failed",
    "phase" : "query",
    "grouped" : true,
    "failed_shards" : [
      {
        "shard" : 0,
        "index" : "movies",
        "node" : "QxD2WNLlS-S3DDH0wY9EVA",
        "reason" : {
          "type" : "illegal_argument_exception",
          "reason" : "Fielddata is disabled on text fields by default. Set fielddata=true on [title] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead."
        }
      }
    ]
  },
  "status" : 400
}

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

Now deleteing the existing index:

curl -XDELETE 127.0.0.1:9200/movies

curl -XPUT 127.0.0.1:9200/_bull --data-binary @movies.json

curl -XGET 127.0.0.1:9200/movies/movie/_search?sort=title.raw&pretty


-------------------------------------------------------------------------------------------------------

more on filters:

Complex filter query:

curl -XGET '127.0.0.1:9200/movies/_search' -d '
{
	"query": {
		"bool": {
			"must": {"match": {"genre": "Sci-Fi"}},
			"must_not": {"match": {"title": "trek"}},
			"filter": {"range": {"year":{"gte": 2010,"lt": 2015}}}
		}
	}
}'

This a boolean query which involves 3 different condiitions....more on this later.


Search for science fiction movies before 1960 sorted by title

curl -XGET '127.0.0.1:9200/movies/_search?sort=title.raw&pretty' -d '
{	
	"query": {
		"bool": {
			"must": {"match": {"genre": "Sci-Fi"}},
			"filter": {"range": {"year":{"lt": 1960}}}
		}
	}
}'


---------------------------------------------------------------------------------------
Fuzzyness: fuzzy matches are about dealing with typos and misspellings by the user

Levenshteins edit distances ----dealing with typos:

1. substitutions of characters interstellar -> intersteller 	types in wrong char by mistake ---here e instead of a
"intersteller" will still match if we are ready to tolerate the Levenshteins edit distance of 1

2. insertions interstellar -> insterstellar

3. deletion interstellar -> interstelar

all of above have edit distances of 1...so if we have search terms having edit distances equal to 1 

So if we specify edit distances in the query we will be able to still match the documents.

Fuzzy query demo:
curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
	"query": {
		"fuzzy": {
			"title": {"value": "intrsteller","fuzziness": 2}
		}
	}
}'

Auto: fuzziness

the auto setting for fuzzyness will tolerate
0  edit distance for 1-2 char strings
1 for 3-5
2 for anything else

examples :

what happens when we have a typo in search:

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
"query": {
	"match": {"title": "intersteller"}
}
}'

response : no docs returned in response

Now using fuzzy query:
curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
	"query": {
		"fuzzy": {
			"title": {"value": "intrsteller","fuzziness": 2}
		}
	}
}'


response :

{
  "took" : 123,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.2798861,
    "hits" : [
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "109487",
        "_score" : 1.2798861,
        "_source" : {
          "id" : "109487",
          "title" : "Interstellar",
          "year" : 2014,
          "genre" : [
            "Sci-Fi",
            "IMAX"
          ]
        }
      }
    ]
  }
}

This above query was able to tolerate a levenshtein edit distance of 1----the fuzziness we specified

----------------------------------------------------------------------------------------------------------

Partial matching:

Prefix queries on strings:

curl -XGEt 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
"query": {
        "prefix": {"year" : "201"}
}
}'

This query would have worked if year was a string field but now its a long---it would match any thing which started with 201

Prefix matches are easy for elasticsearch its pretty quick-----inverted indexes store terms in away that this kind of query is fast.

Wild card queries: works only for string fields

curl -XGEt 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
"query": {
        "wildcard": {"year" : "1*"}
}
}'

This can match any thing which  starts with 1...wildcard (*) can appear anywhere in the search string ...
Regular expressions are also supported for a field

---------------------------------------------------------------------

N-Grams and search as you type:

Search as you type ----We can achive this few different types:

1. Abusing sloppiness:

curl -XGEt 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
	"query": {
		"match_phrase_prefix": {
			"title": {
				"query": "star trek",
				"slop": 10
			}
		}
	}
}

This is easy to do...we dont have to index the data in any particular way to achive this....we only did the query time search-as-you-type

This is an resource intensive operation....we know that prefix matching is a fairly efficient operation in full 
text search as the properties  inverted indexes can make this operation efficient...but still this can be costly.

But if we want to emply this soultion in large scale like google and the likes we need a index time solution.

This is what we would do:

search-as-you-type ---index-time with N-Grams:

Index time with N-grams: its fairly simple:

the term "star":

Unigram : [s,t,a,r] => single letter set which composes the term
Bigram : [st,ta,ar]
Trigram: [sta,tar]
4-gram: [star]

This is applicable to search-as-you-type -----if you treat the input(search query) as N-Gram you can compare it to the terms you have
indexed and see what it matches. So what it means is the n-grams are indexed as terms and when a user types like a single letter "s"
we can match it to unigram [s,t,a,r] and get the doc which matches.

When he types st we can match it with bigram and so and so forth.

What are edge N-grams:

are built only on the begining of each term.

For example if we were building a edge-N-gram for the above we would have Single unigram [s], Single bigram[st], single 
trigram[sta] and single 4-gram[star]

So we index only edge- N-grams and we will have simle way of matching up a combination of letters against the prefixes that
 match for a given search term.

The first step in this process is creating a custom analyzer:

create an autocomplete analyzer:
#######################################################################################
curl -XPUT 127.0.0.1:9200/movies?pretty -d'
{
	"settings": {
		"analysis": {
			"filter": {
				"autocomplete_filter": {
					"type": "edge_ngram",
					"min_gram": 1,
					"max_gram": 20
				}
			},
			"analyzer": {
				"autocomplete": {
					"type": "custom",
					"tokenizer": "standard",
					"filter": ["lowercase", "autocomplete_filter"]
				}
			}
		}
	}
}'

#######################################################################################

The description of filter and analyzer:
"filter": {
				"autocomplete_filter": {
					"type": "edge_ngram",
					"min_gram": 1,
					"max_gram": 20
				}
}

We are declareing the filter of type edge_ngram and we can support upto words upto 20 letters long in our search string

And the analyzer we have standard tokenizer and also added our autocomplete_filter to the list of filters it uses for
tokenizing strings and create search terms before indexing.

Now we need to create amapping with our new analyzer for the title field as follows:

curl -XPUT 127.0.0.1:9200/movies?pretty -d '
{
	"mappings": {
		"movie": {
			"_all": {"enabled":false},
			"properties": {
				"title": {
					"type": "string",
					"analyzer":"autocomplete"
				}
			}
		}
	}
}'

This mapping tells title will be broken down by the autocomplete_filter into edge ngrams terms ranging from 1 to 20 and indexed as 
terms in inverted index.

Now on the query side:

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d'
{
	"query": {
		"match": {
			"title": {
				"query": "sta",
				"analyzer": "standard"
			}
		}
	}
}'

Note This imp***

we are using a different analyzer in the query ---standard. We dont want to split up the query
String into n-grams it should be as it is . 
if we had used autocomplete_analyzer on query side it would have split into s, st, sta and evetything 
starting from the above would have matched 

So the autocomplete_analyzer is only on the indexing side and a
different analyzer is on the query side i.e.. standard.----one of the examles where we have different analyzer
on search and index sides----cool!!!!!!

***************************************************************
Note: very imp ----the field that we are searching on ---the analyzer specified while indexing that field
will be the analyzer which will be applied to the query string as well by default. 
***************************************************************

Need to try out this example -----TODO

getting this analyzer into the elasticsearch:

curl -XPUT 127.0.0.1:9200/movies?pretty --data-binary @autocomplete_analyzer.json

testing the analyzer -----Amazing:
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

curl -XGET '127.0.0.1:9200/movies/_analyze?analyzer=autocomplete&pretty' -d 'Sta' 
{
  "tokens" : [
    {
      "token" : "s",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "st",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "sta",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "<ALPHANUM>",
      "position" : 0
    }
  ]
}

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

Here 'Sta' is broken down into edge n-grams by our autocomplete analyzer
----------------------------------------------------------------------------

Completion suggesters: one more strategy
We can upload a list of all possible completions ahead of time using completion suggesters

curl -XPUT '127.0.0.1:9200/movies/_mapping/movie?pretty' --data-binary @movies_mapping.json

reindex data:

query is as following:

curl -XGET 127.0.0.1:9200/movies/movie/_search?pretty -d '
 {
       "query": {
               "match": {
                       "title": {"query": "sta","analyzer": "standard"}
               }
       }
 }'
{
  "took" : 6,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.3229789,
    "hits" : [
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "122886",
        "_score" : 1.3229789,
        "_source" : {
          "id" : "122886",
          "title" : "Star Wars: Episode VII - The Force Awakens",
          "year" : 2015,
          "genre" : [
            "Action",
            "Adventure",
            "Fantasy",
            "Sci-Fi",
            "IMAX"
          ]
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "135569",
        "_score" : 0.40644988,
        "_source" : {
          "id" : "135569",
          "title" : "Star Trek Beyond",
          "year" : 2016,
          "genre" : [
            "Action",
            "Adventure",
            "Sci-Fi"
          ]
        }
      }
    ]
  }
}





	



		
	




 



	


				





######################################### PART 2 #######################################
This is  Elastic  stack  : Elasticsearch, Logstash,beats framework, Kibana:
This is not ELK stack  : Elasticsearch, Logstash, Kibana:

Note the difference: more details below



Logstash:

Running python script:

python3 MoviesToJson.py >moremovies.json 

in the above we are running a python script (MoviesToJson.py) and redirecting the output to the file (moremovies.json)
> is for direct std output to a file 

Delete the existing index and import the new movies as below:

curl -XDELETE 127.0.0.1:9200/movies

curl -XPUT 127.0.0.1:9200/_bulk --data-binary @moremovies.json

Search for stranger things : URL search:

curl -XGET "127.0.0.1:9200/movies/movie/_search?q=stranger%20things&pretty"


This is not the correct way of dealing with elasticsearch from our programs:

We generally use the client libraries provied by elastic search like Java client lib maintained by 
elastic.co, python, ruby, scala---has many choices, and perl.

 
Now indexed the Movies, Taga and Ratings into elastic search......
TODO write java based script using elastic search client libray to import the movies, taga and the ratings ------currently python
is being used!!!!!!


Logstash:

Logstash is used for moving the data around in servers -----

Logstash sits in between the data and where we want to put it:

like for example:

{files, s3, beats, kafka} --------------> {mongo,elasticsearch,hadoop,aws} : logstash commonly is used to trasfer the contents of a logfile 
being generated on a server somewhere and elastic search cluster. But logstash can trasfer data from multiple sources and trasfer the same to 
many different destination as seen below. And also logstash can injest data from many different sources at the same time and egress the data
to multile destinations at same time------it is flexible.

It does more than plumbing ---- Logstash can do the following to the data as it passes through:

1. Logstash can parse, transform, filter data passsing through it
2. It can derive structure from unstructured data----apply schema----late binding of schema.
3. Annonymize personal data or remove it completely----like usernames, emails, passwords etc...
4. it can do geo-location lookups.
5. it can scale accross many nodes-----important as the amount of data generated in high volume transction distributed systems can be huge.
6. guarantees atleast once delivary.
7. it can absorb throughput from load spikes.

There huge amounts of input source events that can be passed over to logstash and processed like kafka, file system, cloud watch,
couch db, elastic search, windows even logs , github, google pub-sub, etc.

Similarly it has may different destinations where the data can be sent to.

Typical usgge of logstash is ----Publishing weblogs to elasticsearch cluster for visualizing later we can do the following:

1. install a lightweight client called file beat on individual webserver hosts.

2. file beat will send data to logstash cluster ----which will buffer this data and do filter, transformation, geo lookups and other stuff.

3. The logstash then sends the trnsformed , filtered data to elasticsearch fro indexing.

Previously logstash was diretly installed in web server hosts and it directly listened to logs and then do all the transformation , filtering
and other stuff there it self and egrss data to elasticsearch. but this not sucha resilent pattern.........  

The use of beat stack to listen to individual weblogs and then trasferrring data to log stack is modern way of doing it.....this is elastic stack. The old stack where we did not use beats framework is ELK stack.

##########################################################################

Installing and configureing Logstash------First start with ELK stack and then come back to
Elastic stack with beats framework-----later


Configureing the logstash:

logstash.conf: /etc/logstash/conf.d/logstash.conf ------this file has the configuration:

It has a block for input, filter and output:

Sample logstash.conf file below:

input{
	file{
		path=>"/home/fkane/access_log"
		start_position => "beginning"
		ignore_older => 0
	}
}
filter{
	grok{
		match => {"message" => "%{COMBINEDAPACHELOG}" }
	}
	date{
		match => ["timestamp","dd/MMM/yyyy:HH:mm:ss Z"]
	}
}
output{
	elasticsearch{
		hosts => ["localhost:9200"]
	}
	stdout{
		codec => rubydebug
	}
}

The above configuration is self explainatory ----it has 3 sections input, filter and output

Input tells it that the logs to be obtained from a file named access_log and , the los need to be fetched from the begining.
Here we are also telling that the logstash should not consider the age of the file.....it should accept the old files as well.
We are also telling that the logs should be chunked from the beginning----in prod we may not do this we might simple 
get the tail of the logs ----we may not start logstash to fetch logs from the beginning---we will stream the logs as and when a 
new log entry appears.

We may also not need ignore_older flag in prod ...as we may not need old log files to be processed by logstash.



Filter section has 2 filters----grok filter is telling that the	logs are are apache logs and each logentry template is that
of a apache log.

further we are telling that add the time stamp entry for each log entry----in the given format.

Output section tells that the log entries should be egressed to elastic server runnong on localhost:9200 and also th the std output


Installing logstash:

sudo apt-get update && sudo apt-get install logstash

We are going to take a apache log file and the logstash is going to do the etl on it and send it to elasticsearch cluster as defined in the 
above logstash.conf file


starting logstash: all this is done on the server where we installed logstash-----in this setup losgstash is on VM

change dir to /usr/share/logstash ----

run the below:

$$$$$$$$$$$$$$$$$$$$$

bin/logstash -f /etc/logstash/conf.d/logstash.conf
$$$$$$$$$$$$$$$$$$$$$

Now if we note in the console we see that the logstash extracts the different fields in the apache log and constructs
the elasticsearch docs from those fields of the current log entry and sends to elasticsearch....it also
prints the docs to stdout.

now lets see what indices exist on elasticsearch cluster: command as below to explore the indexes in elasticsearch:
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

curl -XGET 127.0.0.1:9200/_cat/indices?v

Output as below:

health status index               uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   shakespeare         aitiYMJVQxWAJgpqF2yYZA   5   1     111396            0     28.9mb         28.9mb
yellow open   series              le3Z5QlzROayLfZzmjnVhA   5   1          8            0     12.8kb         12.8kb
yellow open   logstash-2017.05.02 QRgRbrVwTLKmexntE_CBJw   5   1      16278            0     11.1mb         11.1mb
yellow open   logstash-2017.05.04 xTT7y5rfTECPMoE2i_AdrQ   5   1      16762            0     11.2mb         11.2mb
yellow open   movies              LvxJWkXIQ3eurXzsbNU2TA   5   1       9125            0      1.7mb          1.7mb
yellow open   ratings             gqqbQVvBR3SSgAp9hLL9XA   5   1     100004            0     14.9mb         14.9mb
yellow open   tags                QDSdE9HHTey1GJ_K1C-Q1w   5   1       1296            0    437.7kb        437.7kb
green  open   diary               cGMFOzhyQwecZr-Bfqjv5A   3   0          0            0       573b           573b
yellow open   logstash-2017.05.05 Fn773QszSLiboT_edrDBKQ   5   1      18646            0       12mb           12mb
yellow open   logstash-2017.05.03 ZWJ7GCsbTkOVOym-LfNj1A   5   1      21172            0     11.2mb         11.2mb
yellow open   logstash-2017.05.01 e9J5szsHSP2yYveV4OWraA   5   1      15948            0       11mb           11mb
yellow open   logstash-2017.04.30 8_1h4VvkThGE17UQ_df1Tw   5   1      14166            0      7.8mb          7.8mb

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$


So if we see the above output we will notice we have new indices prefixed with logstash and the dates:

These new indices were created by logstash from the apache_log which it processed. The log entries are grouped in individual
indexes according to the dates on which the corresponding log entries were created.

This way of indexing ie..according to the date when the log entry was appended helps us in deleting the older indexes
and keeping only the once which are new. this will keep us from overloading the cluster.

Now searching one logstash index as follows:
####################################################################################################
curl -XGET 127.0.0.1:9200/logstash-2017.05.02/_search?pretty
{
  "took" : 12,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 16278,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "logstash-2017.05.02",
        "_type" : "logs",
        "_id" : "AWDWFJyTBLZHkAnfOFrp",
        "_score" : 1.0,
        "_source" : {
          "request" : "/2013/05/new-hosek-wilkie-sky-model-in-silverlining-2-8/",
          "agent" : "\"Mozilla/5.0 (compatible; AhrefsBot/5.2; +http://ahrefs.com/robot/)\"",
          "auth" : "-",
          "ident" : "-",
          "verb" : "GET",
          "message" : "217.182.132.155 - - [02/May/2017:00:02:49 +0000] \"GET /2013/05/new-hosek-wilkie-sky-model-in-silverlining-2-8/ HTTP/1.1\" 200 15406 \"-\" \"Mozilla/5.0 (compatible; AhrefsBot/5.2; +http://ahrefs.com/robot/)\"",
          "path" : "/home/santhosh/access_log",
          "referrer" : "\"-\"",
          "@timestamp" : "2017-05-02T00:02:49.000Z",
          "response" : "200",
          "bytes" : "15406",
          "clientip" : "217.182.132.155",
          "@version" : "1",
          "host" : "ubuntu",
          "httpversion" : "1.1",
          "timestamp" : "02/May/2017:00:02:49 +0000"
        }
      },

.......
}

as we can see logstash has extracted various fields from log entries, constructed the documents(structured the log entry)
nd posted to elasticsearch for indexing.

Now trere are may questions as follows: TODO answer each of them and add more questions below

1. How do we parse the application logs like a impression logs in adserver?

2. what if the logs have may different formats like impression, clicks, conversion or a DSP logs
the question is how we can we conditionally parse different log entries having different format ie..different fields etc?

3. ??????????????????????

####################################################################################################	

Where to search

Where to search
You can tell Elasticsearch to look in a specific type of a specific index, as in listing 2.2,
but you can also search in multiple types in the same index, in multiple indices, or in
all indices.
To search in multiple types, use a comma-separated list. For example, to search in
both group and event types, run a command like this:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 curl "localhost:9200/get-together/group,event/_search\
?q=elasticsearch&pretty"

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
You can also search in all types of an index by sending your request to the _search
endpoint of the index’s URL :
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% curl 'localhost:9200/get-together/_search?q=sample&pretty'

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Similar to types, to search in multiple indices, separate them with a comma:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% curl "localhost:9200/get-together,other-index/_search\
?q=elasticsearch&pretty"

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This particular request will fail unless you created other-index in advance. To ignore
such problems, you can add the ignore_unavailable flag in the same way you add
the pretty flag. To search in all indices, omit the index name altogether:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% curl 'localhost:9200/_search?q=elasticsearch&pretty'

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

_all as the index name. This comes in handy when you need to search in a
TIP
single type across all indices as in this example: http://localhost:9200/_all/
event/_search.
This flexibility regarding where to search allows you to organize data in multiple indices
and types, depending on what makes sense for your use case.

this is very important. these scenarios are handy when we want to scale out.
As we know number of shards are fixed during index creation and we will not be able to 
change it later. So if we want to scale beyond the confines of our shards we will have to create a new index and put 
the documents in new index. So while searching we can use above techniques to search for a type in multiple indexes.



###################################################################################################3	

tcp dump on a given port on the server ------this is the node which receives the request:

sudo tcpdump -vv port 9200 and '(tcp-syn|tcp-ack)!=0'

in the above the we are running the command on serer and dumping the request received on the port 9200
###################################################################################################

 





  














 






